# ===========================================
# LocalMind Environment Configuration
# ===========================================
# Copy this file to .env and customize values
# ===========================================

# ----- LLM Configuration -----
# Provider: ollama, openai, or llamacpp
LLM_PROVIDER=ollama

# Base URL for OpenAI-compatible API
# - For Ollama: http://<host>:11434/v1
# - For OpenAI: https://api.openai.com/v1
# - For LlamaCpp: http://<host>:8080/v1
LLM_BASE_URL=http://192.168.1.173:11434/v1

# API key (use "not-required" for local servers like Ollama)
LLM_API_KEY=not-required

# Default model to use
LLM_MODEL=gpt-oss:latest

# ----- Backend Server -----
BACKEND_HOST=127.0.0.1
BACKEND_PORT=52817

# ----- Database -----
DATABASE_PATH=./data/local_mind.db
