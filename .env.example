# ===========================================
# LocalMind Environment Configuration
# ===========================================
# Copy this file to .env and customize values
# ===========================================

# ----- Kamal Deployment -----
# Server IP or hostname for deployment
DEPLOY_SERVER_IP=192.168.1.100

# Optional: Domain name (leave empty for IP-based access)
DEPLOY_DOMAIN=

# GitHub Container Registry password
# Generate a Personal Access Token at: https://github.com/settings/tokens/new?scopes=write:packages
KAMAL_REGISTRY_PASSWORD=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# ----- LLM Configuration -----
# Provider: ollama, openai, gemini, cerebras, claude, mistral, openai_compatible
LLM_PROVIDER=ollama

# Base URL for OpenAI-compatible API
# - For Ollama: http://<host>:11434/v1
# - For OpenAI: https://api.openai.com/v1
# - For LlamaCpp: http://<host>:8080/v1
LLM_BASE_URL=http://192.168.1.173:11434/v1

# API key (use "not-required" for local servers like Ollama)
LLM_API_KEY=not-required

# Default model to use
LLM_MODEL=llama3:instruct

# ----- Backend Server -----
BACKEND_HOST=0.0.0.0
BACKEND_PORT=52817

# ----- Database -----
# For local development: ./data/local_mind.db
# For Docker/Kamal: /app/data/local_mind.db
DATABASE_PATH=./data/local_mind.db
